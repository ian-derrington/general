This summarizes important research and survey papers and topics related to GPT-enabled technology

# Papers / Codebases

## LLM Component concepts
### Scaling
- [The 'Chinchilla' paper](https://arxiv.org/abs/2203.15556) This paper identifies scaling laws that help to understand the volume of data that is needed to obtain 'optimal' performance for a given LLM models size. Use of it in other areas, such as for Llama reveal that the models may have been under trained.

## Agentic GPT
This section describes GPT that has been enabled with more 'agency'.
- [HuggingGPT](https://arxiv.org/pdf/2303.17580.pdf) This paper describes a paradigm where ChatGPT is enabled with the ability to launch other ML models based on input. It does so by creating a Task list, then by identifying appropriate models, and then by executing them.
  - [Github repo known as JARVIS here](https://github.com/microsoft/JARVIS)
-  

